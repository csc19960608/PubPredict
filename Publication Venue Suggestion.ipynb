{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication Venue Suggestion in Heterogenous Graphs\n",
    "\n",
    "Zubin Pahuja ([zpahuja2@illinois.edu](mailto:zpahuja2@illinois.edu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture\n",
    "\n",
    "We build a machine-learning model to help suggest a publication venue to submit a research paper an individual researcher has written. While an intuitive baseline model would use the textual content such as paper title, our model utilizes additional features derived from a heterogenous information network on the DBLP dataset, that is constructed when scientific literature is published in venues and links are formed by publication and additional links are formed by papers citing other papers. We will then analyze the heterogenous features used in the model to understand the importance of each feature in the recommendation process.\n",
    "\n",
    "We create a feature vector consisting of a bag-of-words text representation of the DBLP title for each paper, use these title-text features to learn a model that uses text-features to predict publication venue of a paper.\n",
    "\n",
    "Next, we additionally create meta-path features across the heterogenous network such as a bag-of-words representation of the publication venues of each cited paper. (`Paper1 → cites → Paper2 → published_in → Venue1`). The venues of the cited papers can be used as additional features to the text-features to learn a model that uses text features and HIN-based features to predict Paper1's publication venue.\n",
    "\n",
    "We compare prediction performance of both models using F1-Score on a test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import logging\n",
    "import warnings\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from os.path import join\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Set**\n",
    "\n",
    "* The labels file contains a list of publication venues that are your target venues to suggest for a paper.\n",
    "* The training/validation file contain five tab-delimited columns:\n",
    "\n",
    "    ```Paper_Id    Paper_title    Publication_venue    Cited_Papers    Cited_Papers_Venues```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/zubin/Desktop/PubPredict/data'\n",
    "train_filepath = join(data_path, 'train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    \"\"\"\n",
    "    :return: dictionary of words: True for stopwords that occur less than 5 times in train set.\n",
    "    \"\"\"\n",
    "    stopwords = {}\n",
    "    word_count = {}\n",
    "        \n",
    "    with open(train_filepath, 'r', encoding='utf-8') as train_file:\n",
    "        for line in train_file:\n",
    "            row = line.rstrip().split('\\t')\n",
    "            title = row[1].lower()\n",
    "            title = re.sub(r'[^a-z\\s-]', '', title) # remove all but a-z characters, spaces and hyphen.\n",
    "            \n",
    "            # count tokens\n",
    "            for token in title.split():\n",
    "                word_count[token] = word_count.get(token, 0) + 1\n",
    "    \n",
    "    for token in word_count.keys():\n",
    "        if word_count[token] < 5:\n",
    "            stopwords[token] = True\n",
    "    \n",
    "    return stopwords\n",
    "\n",
    "stopwords = get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    \"\"\"\n",
    "    Lowercase title, remove non-alphabet characters and tokens that occur less than 5 times in train set.\n",
    "    :param filename: the filename of .TSV data set.\n",
    "    \"\"\"\n",
    "    \n",
    "    filepath = join(data_path, filename)\n",
    "    output_filepath = join(data_path, 'cleaned_' + filename)\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as tsv_file, open(output_filepath, 'w') as cleaned_tsvout:\n",
    "        cleaned_tsv_writer = csv.writer(cleaned_tsvout, delimiter='\\t')\n",
    "\n",
    "        for line in tsv_file:\n",
    "            row = line.rstrip().split('\\t')                \n",
    "            title = row[1].lower()\n",
    "            title = re.sub(r'[^a-z\\s\\t-]', '', title) # remove all but a-z characters, spaces and hyphen.\n",
    "            title = ' '.join(filter(lambda x: not stopwords.get(x, False),  title.split('\\s'))) # remove infrequent tokens.\n",
    "            row[1] = title\n",
    "            cleaned_tsv_writer.writerow(row)\n",
    "\n",
    "clean_data('train.txt')\n",
    "clean_data('train_subset.txt')\n",
    "clean_data('validation.txt')\n",
    "clean_data('test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create text-based feature vector for each paper\n",
    "\n",
    "For each paper in the training, validation, and test set, title attribute is used to create a bag-of-words feature vector.\n",
    "\n",
    "Additionally publication venue of each paper is encoded into an integer between 0 to (Number of Venues - 1).\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation\n",
    "\n",
    "2. http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "3. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "labels = []\n",
    "\n",
    "with open(join(data_path, 'labels.txt'), 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        labels.append(line.rstrip())\n",
    "\n",
    "encoded_labels = le.fit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358429, 112092) (358429,)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_data(filename, ngram_range=(1, 2), tfidf=True, saveToDisk=False, train=True, vectorizer=None, transformer=None, test=False):\n",
    "    \"\"\"\n",
    "    Compute count feature vectors for text data and encode labels.\n",
    "    :param filename: the filename of .TSV data set.\n",
    "    :param ngram_range: lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    :param tfidf: normalize count vectors using tf-idf (default=True).\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, tokenizer=lambda a: a.split(' '))\n",
    "        transformer = TfidfTransformer(smooth_idf=False)\n",
    "    else:\n",
    "        assert(vectorizer is not None)\n",
    "        assert(transformer is not None)\n",
    "    \n",
    "    corpus = []\n",
    "    Y = []\n",
    "    \n",
    "    filepath = join(data_path, filename)\n",
    "    output_filepath = join(data_path, 'text_features_' + filename)\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as tsv_file:\n",
    "        for line in tsv_file:\n",
    "            row = line.rstrip().split('\\t')\n",
    "            try:\n",
    "                corpus.append(row[1])\n",
    "                Y.append(row[2])\n",
    "            except:\n",
    "                print(row)\n",
    "                continue\n",
    "        \n",
    "        if train:\n",
    "            X = vectorizer.fit_transform(corpus)\n",
    "            if tfidf:\n",
    "                X = transformer.fit_transform(X)\n",
    "        else:\n",
    "            X = vectorizer.transform(corpus)\n",
    "            if tfidf:\n",
    "                X = transformer.transform(X)\n",
    "                    \n",
    "        if not test:\n",
    "            Y = le.transform(Y)\n",
    "            print (X.shape, Y.shape)\n",
    "        else:\n",
    "            Y = np.zeros(len(Y))\n",
    "        \n",
    "        if saveToDisk:\n",
    "            Y2 = Y.reshape((Y.shape[0], 1))\n",
    "            Z = hstack((X, Y2))\n",
    "            np.savetxt(output_filepath, Z.A, delimiter=',')\n",
    "        \n",
    "        else:\n",
    "            if train:\n",
    "                return X, Y, vectorizer, transformer\n",
    "            return X, Y\n",
    "        \n",
    "train_X, train_Y, train_vectorizer, train_transformer = vectorize_data('cleaned_train.txt', ngram_range=(1, 1), tfidf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 112092) (10,)\n"
     ]
    }
   ],
   "source": [
    "vectorize_data('cleaned_train_subset.txt', ngram_range=(1, 1), saveToDisk=True, train=False, vectorizer=train_vectorizer, transformer=train_transformer, tfidf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifier model for predicting venue given title features\n",
    "\n",
    "Using SGDClassifier. \n",
    "\n",
    "**References:**\n",
    "\n",
    "1. http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "2. http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "3. http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "\n",
    "\n",
    "Performance is evaluated on the validation set by calculating micro and macro F1 score. Additionally the precision and recall are reported per venue (class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44629, 112092) (44629,)\n"
     ]
    }
   ],
   "source": [
    "validation_X, validation_Y = vectorize_data('cleaned_validation.txt', ngram_range=(1, 1), train=False, vectorizer=train_vectorizer, transformer=train_transformer, tfidf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "s = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Macro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19998591387183903"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf2.predict(validation_X)\n",
    "f1_score(validation_Y, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30419682269376414"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(validation_Y, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29035163275803644"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(validation_Y, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class-wise Precision Recall on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                                            aaai       0.05      0.15      0.08       528\n",
      "                                           aamas       0.37      0.28      0.32       349\n",
      "                                             acc       0.00      0.00      0.00       109\n",
      "                                  acm_multimedia       0.28      0.16      0.20       379\n",
      "                               acm_trans._graph.       0.00      0.00      0.00         1\n",
      "                                           amcis       0.31      0.28      0.29       665\n",
      "                                            amia       0.57      0.42      0.49       121\n",
      "                                         asp-dac       0.36      0.04      0.07       319\n",
      "                                  bioinformatics       0.00      0.00      0.00        17\n",
      "                                             cdc       0.48      0.58      0.53       843\n",
      "                                             chi       0.32      0.22      0.26       418\n",
      "                          chi_extended_abstracts       0.31      0.42      0.35       568\n",
      "                                            cikm       0.20      0.08      0.11       377\n",
      "                                          cogsci       0.48      0.56      0.51       434\n",
      "                                          coling       0.40      0.25      0.31       367\n",
      "                                     commun._acm       0.33      0.03      0.05        68\n",
      "                                         compsac       0.39      0.05      0.08       296\n",
      "                            comput._graph._forum       0.00      0.00      0.00         0\n",
      "                                      comput._j.       0.00      0.00      0.00        19\n",
      "                         computer_communications       0.00      0.00      0.00         0\n",
      "                               computer_networks       0.25      0.50      0.33         2\n",
      "                                            corr       0.00      0.00      0.00         0\n",
      "                                            cvpr       0.18      0.17      0.17       593\n",
      "                                             dac       0.23      0.25      0.24       446\n",
      "                                            date       0.25      0.15      0.19       490\n",
      "                                            ecai       0.03      0.01      0.02       187\n",
      "                                            ecis       0.32      0.17      0.22       385\n",
      "                                            embc       0.41      0.28      0.33       342\n",
      "                encyclopedia_of_database_systems       0.29      0.82      0.42       301\n",
      "                                            etfa       0.36      0.23      0.28       255\n",
      "                                      eurospeech       0.28      0.04      0.07       318\n",
      "                                         eusipco       0.17      0.14      0.15       852\n",
      "                              expert_syst._appl.       0.00      0.00      0.00         0\n",
      "                                            focs       0.36      0.16      0.22       259\n",
      "                                            fskd       0.28      0.04      0.07       289\n",
      "                                 fundam._inform.       0.00      0.00      0.00         1\n",
      "                                          fusion       0.42      0.40      0.41       286\n",
      "                                       fuzz-ieee       0.51      0.66      0.58       490\n",
      "                                           gecco       0.45      0.34      0.39       385\n",
      "                                        globecom       0.19      0.15      0.17      1323\n",
      "                                           hicss       0.48      0.27      0.35       981\n",
      "                                           icalt       0.56      0.47      0.51       338\n",
      "                                          icarcv       0.00      0.00      0.00       269\n",
      "                                          icassp       0.19      0.27      0.23      1662\n",
      "                                             icc       0.15      0.20      0.17      1380\n",
      "                                           iccad       0.23      0.10      0.14       302\n",
      "                                            iccs       0.32      0.20      0.25       211\n",
      "                                            iccv       0.12      0.09      0.10       272\n",
      "                                           icdar       0.56      0.68      0.62       284\n",
      "                                            icde       0.21      0.20      0.21       343\n",
      "                                           icecs       0.17      0.03      0.05       293\n",
      "                                            icip       0.26      0.32      0.29       947\n",
      "                                            icis       0.27      0.04      0.07       407\n",
      "                                            icmc       0.54      0.73      0.62       435\n",
      "                                            icme       0.20      0.11      0.14       540\n",
      "                                            icml       0.16      0.12      0.14       264\n",
      "                                            icnc       0.26      0.05      0.08       210\n",
      "                                            icpr       0.12      0.02      0.04       456\n",
      "                                            icra       0.39      0.54      0.45      1793\n",
      "                                            icse       0.36      0.33      0.34       324\n",
      "                                           icslp       0.23      0.03      0.06       283\n",
      "                                   ieee_computer       0.00      0.00      0.00        18\n",
      "       ieee_congress_on_evolutionary_computation       0.41      0.37      0.39       396\n",
      "ieee_journal_on_selected_areas_in_communications       0.00      0.00      0.00         1\n",
      "                                   ieee_software       0.00      0.00      0.00         4\n",
      "                           ieee_trans._computers       0.00      0.00      0.00        19\n",
      "                  ieee_trans._information_theory       0.00      0.00      0.00         0\n",
      "                    ieee_trans._knowl._data_eng.       0.00      0.00      0.00        42\n",
      "             ieee_trans._parallel_distrib._syst.       0.00      0.00      0.00         1\n",
      "         ieee_trans._pattern_anal._mach._intell.       0.00      0.00      0.00         0\n",
      "                       ieee_trans._software_eng.       0.05      0.02      0.03        46\n",
      "                                          igarss       0.72      0.87      0.79      1325\n",
      "                                           ijcai       0.09      0.26      0.13       752\n",
      "                                           ijcnn       0.35      0.26      0.30       543\n",
      "                             inf._process._lett.       0.17      0.05      0.07        21\n",
      "                                       inf._sci.       0.00      0.00      0.00         0\n",
      "                                         infocom       0.27      0.26      0.26       690\n",
      "                             int._cmg_conference       0.62      0.64      0.63       358\n",
      "                                     interspeech       0.46      0.71      0.56      1252\n",
      "                                           ipdps       0.32      0.30      0.31       449\n",
      "                                            iros       0.32      0.23      0.27      1399\n",
      "                                            isbi       0.46      0.63      0.53       417\n",
      "                                           iscas       0.38      0.42      0.40       889\n",
      "                                            iscc       0.09      0.03      0.05       323\n",
      "                                            isit       0.43      0.56      0.49       460\n",
      "                                             itc       0.55      0.72      0.63       452\n",
      "                                          j._acm       0.00      0.00      0.00        32\n",
      "                    j._parallel_distrib._comput.       0.00      0.00      0.00         1\n",
      "                                   j._symb._log.       0.00      0.00      0.00         1\n",
      "                 journal_of_systems_and_software       0.00      0.00      0.00         0\n",
      "                                             kdd       0.18      0.14      0.16       239\n",
      "                                             lcn       0.26      0.06      0.10       286\n",
      "                                            lrec       0.52      0.61      0.56       461\n",
      "                          multimedia_tools_appl.       0.00      0.00      0.00         1\n",
      "                                      neuroimage       0.00      0.00      0.00         0\n",
      "                                            nips       0.17      0.31      0.22       588\n",
      "                                           pacis       0.42      0.04      0.07       292\n",
      "                             pattern_recognition       0.00      0.00      0.00         9\n",
      "                                           pdpta       0.33      0.09      0.15       301\n",
      "                                           pimrc       0.24      0.14      0.18       898\n",
      "                                           robio       0.32      0.09      0.14       404\n",
      "                                             sac       0.10      0.12      0.11       577\n",
      "                                 siam_j._comput.       0.00      0.00      0.00        14\n",
      "                                          sigcse       0.66      0.70      0.68       392\n",
      "                                           sigir       0.37      0.26      0.31       351\n",
      "                               sigmod_conference       0.18      0.14      0.16       299\n",
      "                                             smc       0.17      0.09      0.12       871\n",
      "                                            soda       0.34      0.29      0.31       292\n",
      "                           softw.,_pract._exper.       0.00      0.00      0.00        11\n",
      "                                            stoc       0.36      0.21      0.26       311\n",
      "                             theor._comput._sci.       0.11      0.07      0.08        15\n",
      "                                     vlsi_design       0.22      0.06      0.09       271\n",
      "                                        vtc_fall       0.19      0.03      0.05       521\n",
      "                                      vtc_spring       0.21      0.09      0.13       604\n",
      "                                            wcnc       0.14      0.09      0.11       781\n",
      "                    winter_simulation_conference       0.54      0.72      0.62       873\n",
      "\n",
      "                                     avg / total       0.31      0.30      0.29     44629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_Y, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, _ = vectorize_data('cleaned_test.txt', ngram_range=(1, 1), test=True, train=False, vectorizer=train_vectorizer, transformer=train_transformer, tfidf=False)\n",
    "y_pred = clf2.predict(test_X)\n",
    "test_pred_Y = le.inverse_transform(y_pred)\n",
    "test_data = np.loadtxt(join(data_path, 'cleaned_test.txt'), dtype=str, delimiter='\\t')\n",
    "paper_ids = test_data[:,0]\n",
    "test_predictions = np.vstack((paper_ids, test_pred_Y)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    join(data_path,'text_feature_predictions.txt'), \n",
    "    test_predictions, \n",
    "    delimiter='\\t',\n",
    "    fmt='%s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using HIN features to supplement text-features\n",
    "\n",
    "The cited venue string is concatenated to the title to create a bag-of-words feature of title + venues of cited papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hin_vectorize_data(filename, ngram_range=(1, 2), tfidf=True, saveToDisk=False, train=True, vectorizer=None, transformer=None, test=False):\n",
    "    \"\"\"\n",
    "    Compute count feature vectors for text data + cited venue strings and encode labels.\n",
    "    :param filename: the filename of .TSV data set.\n",
    "    :param ngram_range: lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    :param tfidf: normalize count vectors using tf-idf (default=True).\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, tokenizer=lambda a: a.split(' '))\n",
    "        transformer = TfidfTransformer(smooth_idf=False)\n",
    "    else:\n",
    "        assert(vectorizer is not None)\n",
    "        assert(transformer is not None)\n",
    "    \n",
    "    corpus = []\n",
    "    Y = []\n",
    "    \n",
    "    filepath = join(data_path, filename)\n",
    "    output_filepath = join(data_path, 'text_hin_features_' + filename)\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as tsv_file:\n",
    "        for line in tsv_file:\n",
    "            row = line.rstrip().split('\\t')\n",
    "            try:\n",
    "                corpus.append(row[1] + ' ' + row[4])\n",
    "                Y.append(row[2])\n",
    "            except:\n",
    "                print(row)\n",
    "                continue\n",
    "        \n",
    "        if train:\n",
    "            X = vectorizer.fit_transform(corpus)\n",
    "            if tfidf:\n",
    "                X = transformer.fit_transform(X)\n",
    "        else:\n",
    "            X = vectorizer.transform(corpus)\n",
    "            if tfidf:\n",
    "                X = transformer.transform(X)\n",
    "                    \n",
    "        if not test:\n",
    "            Y = le.transform(Y)\n",
    "            print (X.shape, Y.shape)\n",
    "        else:\n",
    "            Y = np.zeros(len(Y))\n",
    "        \n",
    "        if saveToDisk:\n",
    "            Y2 = Y.reshape((Y.shape[0], 1))\n",
    "            Z = hstack((X, Y2))\n",
    "            np.savetxt(output_filepath, Z.A, delimiter=',')\n",
    "        \n",
    "        else:\n",
    "            if train:\n",
    "                return X, Y, vectorizer, transformer\n",
    "            return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358429, 112168) (358429,)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, train_vectorizer, train_transformer = hin_vectorize_data('cleaned_train.txt', ngram_range=(1, 1), tfidf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 112168) (10,)\n"
     ]
    }
   ],
   "source": [
    "hin_vectorize_data('cleaned_train_subset.txt', ngram_range=(1, 1), tfidf=False, saveToDisk=True, train=False, vectorizer=train_vectorizer, transformer=train_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44629, 112168) (44629,)\n"
     ]
    }
   ],
   "source": [
    "validation_X, validation_Y = hin_vectorize_data('cleaned_validation.txt', ngram_range=(1, 1), tfidf=False, train=False, vectorizer=train_vectorizer, transformer=train_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "s = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Macro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7620302763937719"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf2.predict(validation_X)\n",
    "f1_score(validation_Y, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824329471868067"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(validation_Y, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824675480968728"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(validation_Y, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class-wise Precision Recall on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                                            aaai       1.00      0.93      0.96       528\n",
      "                                           aamas       1.00      1.00      1.00       349\n",
      "                                             acc       1.00      1.00      1.00       109\n",
      "                                  acm_multimedia       1.00      1.00      1.00       379\n",
      "                               acm_trans._graph.       0.00      0.00      0.00         1\n",
      "                                           amcis       1.00      1.00      1.00       665\n",
      "                                            amia       1.00      1.00      1.00       121\n",
      "                                         asp-dac       1.00      1.00      1.00       319\n",
      "                                  bioinformatics       1.00      1.00      1.00        17\n",
      "                                             cdc       1.00      1.00      1.00       843\n",
      "                                             chi       1.00      1.00      1.00       418\n",
      "                          chi_extended_abstracts       1.00      1.00      1.00       568\n",
      "                                            cikm       0.92      0.86      0.89       377\n",
      "                                          cogsci       1.00      1.00      1.00       434\n",
      "                                          coling       1.00      1.00      1.00       367\n",
      "                                     commun._acm       0.43      0.09      0.15        68\n",
      "                                         compsac       1.00      1.00      1.00       296\n",
      "                            comput._graph._forum       0.00      0.00      0.00         0\n",
      "                                      comput._j.       0.33      0.05      0.09        19\n",
      "                         computer_communications       0.00      0.00      0.00         0\n",
      "                               computer_networks       0.33      0.50      0.40         2\n",
      "                                            corr       0.00      0.00      0.00         0\n",
      "                                            cvpr       0.99      0.89      0.94       593\n",
      "                                             dac       1.00      1.00      1.00       446\n",
      "                                            date       1.00      1.00      1.00       490\n",
      "                                            ecai       1.00      0.98      0.99       187\n",
      "                                            ecis       1.00      1.00      1.00       385\n",
      "                                            embc       1.00      1.00      1.00       342\n",
      "                encyclopedia_of_database_systems       0.97      1.00      0.99       301\n",
      "                                            etfa       1.00      1.00      1.00       255\n",
      "                                      eurospeech       1.00      1.00      1.00       318\n",
      "                                         eusipco       1.00      1.00      1.00       852\n",
      "                              expert_syst._appl.       0.00      0.00      0.00         0\n",
      "                                            focs       1.00      1.00      1.00       259\n",
      "                                            fskd       1.00      1.00      1.00       289\n",
      "                                 fundam._inform.       0.00      0.00      0.00         1\n",
      "                                          fusion       0.99      1.00      1.00       286\n",
      "                                       fuzz-ieee       1.00      1.00      1.00       490\n",
      "                                           gecco       1.00      1.00      1.00       385\n",
      "                                        globecom       1.00      1.00      1.00      1323\n",
      "                                           hicss       1.00      1.00      1.00       981\n",
      "                                           icalt       1.00      1.00      1.00       338\n",
      "                                          icarcv       1.00      1.00      1.00       269\n",
      "                                          icassp       1.00      1.00      1.00      1662\n",
      "                                             icc       1.00      1.00      1.00      1380\n",
      "                                           iccad       1.00      1.00      1.00       302\n",
      "                                            iccs       1.00      1.00      1.00       211\n",
      "                                            iccv       0.85      0.79      0.82       272\n",
      "                                           icdar       1.00      1.00      1.00       284\n",
      "                                            icde       0.77      0.74      0.76       343\n",
      "                                           icecs       1.00      1.00      1.00       293\n",
      "                                            icip       1.00      1.00      1.00       947\n",
      "                                            icis       1.00      1.00      1.00       407\n",
      "                                            icmc       1.00      1.00      1.00       435\n",
      "                                            icme       1.00      1.00      1.00       540\n",
      "                                            icml       1.00      0.94      0.97       264\n",
      "                                            icnc       1.00      1.00      1.00       210\n",
      "                                            icpr       1.00      1.00      1.00       456\n",
      "                                            icra       1.00      1.00      1.00      1793\n",
      "                                            icse       1.00      1.00      1.00       324\n",
      "                                           icslp       1.00      1.00      1.00       283\n",
      "                                   ieee_computer       0.00      0.00      0.00        18\n",
      "       ieee_congress_on_evolutionary_computation       1.00      1.00      1.00       396\n",
      "ieee_journal_on_selected_areas_in_communications       0.00      0.00      0.00         1\n",
      "                                   ieee_software       0.00      0.00      0.00         4\n",
      "                           ieee_trans._computers       0.00      0.00      0.00        19\n",
      "                  ieee_trans._information_theory       0.00      0.00      0.00         0\n",
      "                    ieee_trans._knowl._data_eng.       0.36      0.10      0.15        42\n",
      "             ieee_trans._parallel_distrib._syst.       0.00      0.00      0.00         1\n",
      "         ieee_trans._pattern_anal._mach._intell.       0.43      0.13      0.20        46\n",
      "                       ieee_trans._software_eng.       1.00      1.00      1.00      1325\n",
      "                                          igarss       1.00      1.00      1.00       752\n",
      "                                           ijcai       1.00      1.00      1.00       543\n",
      "                                           ijcnn       0.14      0.05      0.07        21\n",
      "                             inf._process._lett.       0.00      0.00      0.00         0\n",
      "                                       inf._sci.       1.00      1.00      1.00       690\n",
      "                                         infocom       1.00      1.00      1.00       358\n",
      "                             int._cmg_conference       1.00      1.00      1.00      1252\n",
      "                                     interspeech       1.00      1.00      1.00       449\n",
      "                                           ipdps       1.00      1.00      1.00      1399\n",
      "                                            iros       1.00      1.00      1.00       417\n",
      "                                            isbi       0.78      0.99      0.87       889\n",
      "                                           iscas       1.00      1.00      1.00       323\n",
      "                                            iscc       1.00      1.00      1.00       460\n",
      "                                            isit       0.96      0.92      0.94       452\n",
      "                                             itc       0.11      0.12      0.11        32\n",
      "                                          j._acm       0.00      0.00      0.00         1\n",
      "                    j._parallel_distrib._comput.       0.00      0.00      0.00         1\n",
      "                                   j._symb._log.       0.00      0.00      0.00         0\n",
      "                 journal_of_systems_and_software       1.00      1.00      1.00       239\n",
      "                                             kdd       1.00      1.00      1.00       286\n",
      "                                             lcn       1.00      1.00      1.00       461\n",
      "                                            lrec       0.03      1.00      0.05         1\n",
      "                          multimedia_tools_appl.       1.00      1.00      1.00       588\n",
      "                                      neuroimage       1.00      1.00      1.00       292\n",
      "                                            nips       0.56      1.00      0.72         9\n",
      "                                           pacis       1.00      1.00      1.00       301\n",
      "                             pattern_recognition       1.00      1.00      1.00       898\n",
      "                                           pdpta       1.00      1.00      1.00       404\n",
      "                                           pimrc       1.00      1.00      1.00       577\n",
      "                                           robio       0.00      0.00      0.00        14\n",
      "                                             sac       1.00      1.00      1.00       392\n",
      "                                 siam_j._comput.       0.92      0.89      0.91       351\n",
      "                                          sigcse       0.79      0.78      0.79       299\n",
      "                                           sigir       1.00      1.00      1.00       871\n",
      "                               sigmod_conference       1.00      1.00      1.00       292\n",
      "                                             smc       0.33      0.09      0.14        11\n",
      "                                            soda       1.00      0.99      1.00       311\n",
      "                           softw.,_pract._exper.       0.00      0.00      0.00        15\n",
      "                                            stoc       1.00      1.00      1.00       271\n",
      "                             theor._comput._sci.       1.00      1.00      1.00       521\n",
      "                                     vlsi_design       1.00      1.00      1.00       604\n",
      "                                        vtc_fall       1.00      1.00      1.00       781\n",
      "                                      vtc_spring       1.00      1.00      1.00       873\n",
      "\n",
      "                                     avg / total       0.98      0.98      0.98     44629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_Y, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, _ = hin_vectorize_data('cleaned_test.txt', ngram_range=(1, 1), tfidf=False, test=True, train=False, vectorizer=train_vectorizer, transformer=train_transformer)\n",
    "y_pred = clf2.predict(test_X)\n",
    "test_pred_Y = le.inverse_transform(y_pred)\n",
    "test_data = np.loadtxt(join(data_path, 'cleaned_test.txt'), dtype=str, delimiter='\\t')\n",
    "paper_ids = test_data[:,0]\n",
    "test_predictions = np.vstack((paper_ids, test_pred_Y)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    join(data_path,'text_hin_feature_predictions.txt'), \n",
    "    test_predictions, \n",
    "    delimiter='\\t',\n",
    "    fmt='%s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of papers published in venues that they cite papers at is 96.16392928364965\n"
     ]
    }
   ],
   "source": [
    "validation_data = np.loadtxt(join(data_path, 'cleaned_validation.txt'), dtype=str, delimiter='\\t')\n",
    "venues = validation_data[:,2]\n",
    "cited_venues = [venues.split(' ') for venues in validation_data[:,4]]\n",
    "\n",
    "ctr = 0.\n",
    "for i in range(len(venues)):\n",
    "    if venues[i] in cited_venues[i]:\n",
    "        ctr += 1.\n",
    "\n",
    "print(\"Percentage of papers published in venues that they cite papers at is {}\".format(ctr * 100./len(venues)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above statistic shows how important cited venues are because papers are usually published in the same venue as the papers they cite.**\n",
    "\n",
    "We observe that combining HIN-based features with text-based features greatly outperforms model that uses text-based features alone at both precision, recall and hence F-1 score. This is because textual content of title hardly gives us any information about the venue it may be suitable for, as well as there are several venues for the same research domain. It is plausible that author cites his own paper, reads papers of a venue and submits to the venue of his/her preference. Cited venue also demonstrates the research domain of paper better than textual content of its title. This is because titles are sometimes intended to be catchy but are not very informative.\n",
    "\n",
    "Therefore, heterogenous information networks contain richer information than simple text-based model. Concatenating cited venue strings in title essentially adds them to tokens in bag-of-words vector. Since venue names are unique and distinct from words found in vocabulary of paper titles, therefore their occurence in title name essentially serves as concatenating title bag-of-words vector with one-hot-vector-encoding of cited paper's venues. Thus, our model incorporates the meta-path `Paper1 → cites → Paper2 → published_in → Venue1`. This illustrates that more than content, context matters as well. I believe including more metapaths will enrich this model even more.\n",
    "\n",
    "We also observe in later section, even with word embeddings, text is much less important than cited venues in determining venue of a publication.\n",
    "\n",
    "The downside of bag-of-words unigram model is:\n",
    "\n",
    "1. Features vectors are rather large and sparse, which results in slow training\n",
    "2. Our model only uses word counts and for example, cannot differentiate between a statement and a question that use the same words, or a random permutation of the same words/ phrases which could mean something entirely else. Therefore, a better idea would be to use bi-grams or tri-grams, but this would explode the feature vector length.\n",
    "3. Text features used are a simple bag-of words, as such similar words are not considered when doing classification. Contextual similarity can be incorporated using word embeddings.\n",
    "\n",
    "Ways to improve:\n",
    "\n",
    "1. Add more heterogeneity such as author nodes which will encode author's preferences to publish at a certain venue.\n",
    "2. Add more metapaths such as ```Paper-> Keywords -> Venue```\n",
    "3. Longer metapaths that include cited venues of cited papers as well.\n",
    "4. Better data cleaning such as removing stopwords.\n",
    "5. Reduce feature vector length by using word embeddings such as word2vec.\n",
    "6. Count Vectors should be tf-idf weighted since not all words carry same information.\n",
    "7. Rather than unigram bag of words model, use bi-grams or tri-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. More Sophisticated Features\n",
    "\n",
    "The text features used are a simple bag-of words, as such similar words are not considered when doing classification. Word embedding with word2vec is performed to utilize embedding features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt(join(data_path, 'cleaned_train.txt'), dtype=str, delimiter='\\t')\n",
    "validation_data = np.loadtxt(join(data_path, 'cleaned_validation.txt'), dtype=str, delimiter='\\t')\n",
    "test_data = np.loadtxt(join(data_path, 'cleaned_test.txt'), dtype=str, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-01 17:22:48,280 : INFO : collecting all words and their counts\n",
      "2018-05-01 17:22:48,282 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-01 17:22:48,306 : INFO : PROGRESS: at sentence #10000, processed 94688 words, keeping 13386 word types\n",
      "2018-05-01 17:22:48,329 : INFO : PROGRESS: at sentence #20000, processed 190049 words, keeping 20160 word types\n",
      "2018-05-01 17:22:48,361 : INFO : PROGRESS: at sentence #30000, processed 284608 words, keeping 25601 word types\n",
      "2018-05-01 17:22:48,414 : INFO : PROGRESS: at sentence #40000, processed 379019 words, keeping 30240 word types\n",
      "2018-05-01 17:22:48,450 : INFO : PROGRESS: at sentence #50000, processed 474081 words, keeping 34482 word types\n",
      "2018-05-01 17:22:48,489 : INFO : PROGRESS: at sentence #60000, processed 568406 words, keeping 38446 word types\n",
      "2018-05-01 17:22:48,539 : INFO : PROGRESS: at sentence #70000, processed 663009 words, keeping 42211 word types\n",
      "2018-05-01 17:22:48,573 : INFO : PROGRESS: at sentence #80000, processed 758268 words, keeping 45648 word types\n",
      "2018-05-01 17:22:48,608 : INFO : PROGRESS: at sentence #90000, processed 853726 words, keeping 49055 word types\n",
      "2018-05-01 17:22:48,645 : INFO : PROGRESS: at sentence #100000, processed 948877 words, keeping 52213 word types\n",
      "2018-05-01 17:22:48,686 : INFO : PROGRESS: at sentence #110000, processed 1044116 words, keeping 55290 word types\n",
      "2018-05-01 17:22:48,719 : INFO : PROGRESS: at sentence #120000, processed 1139489 words, keeping 58202 word types\n",
      "2018-05-01 17:22:48,749 : INFO : PROGRESS: at sentence #130000, processed 1234580 words, keeping 61117 word types\n",
      "2018-05-01 17:22:48,781 : INFO : PROGRESS: at sentence #140000, processed 1329260 words, keeping 63916 word types\n",
      "2018-05-01 17:22:48,821 : INFO : PROGRESS: at sentence #150000, processed 1423977 words, keeping 66697 word types\n",
      "2018-05-01 17:22:48,850 : INFO : PROGRESS: at sentence #160000, processed 1519100 words, keeping 69235 word types\n",
      "2018-05-01 17:22:48,883 : INFO : PROGRESS: at sentence #170000, processed 1613709 words, keeping 71799 word types\n",
      "2018-05-01 17:22:48,911 : INFO : PROGRESS: at sentence #180000, processed 1708278 words, keeping 74203 word types\n",
      "2018-05-01 17:22:48,941 : INFO : PROGRESS: at sentence #190000, processed 1803768 words, keeping 76578 word types\n",
      "2018-05-01 17:22:48,972 : INFO : PROGRESS: at sentence #200000, processed 1899033 words, keeping 78916 word types\n",
      "2018-05-01 17:22:49,001 : INFO : PROGRESS: at sentence #210000, processed 1993429 words, keeping 81328 word types\n",
      "2018-05-01 17:22:49,027 : INFO : PROGRESS: at sentence #220000, processed 2088161 words, keeping 83607 word types\n",
      "2018-05-01 17:22:49,061 : INFO : PROGRESS: at sentence #230000, processed 2183203 words, keeping 85933 word types\n",
      "2018-05-01 17:22:49,098 : INFO : PROGRESS: at sentence #240000, processed 2277950 words, keeping 88113 word types\n",
      "2018-05-01 17:22:49,135 : INFO : PROGRESS: at sentence #250000, processed 2373279 words, keeping 90387 word types\n",
      "2018-05-01 17:22:49,167 : INFO : PROGRESS: at sentence #260000, processed 2468842 words, keeping 92521 word types\n",
      "2018-05-01 17:22:49,199 : INFO : PROGRESS: at sentence #270000, processed 2563247 words, keeping 94655 word types\n",
      "2018-05-01 17:22:49,235 : INFO : PROGRESS: at sentence #280000, processed 2658373 words, keeping 96728 word types\n",
      "2018-05-01 17:22:49,263 : INFO : PROGRESS: at sentence #290000, processed 2752948 words, keeping 98810 word types\n",
      "2018-05-01 17:22:49,288 : INFO : PROGRESS: at sentence #300000, processed 2847407 words, keeping 100889 word types\n",
      "2018-05-01 17:22:49,315 : INFO : PROGRESS: at sentence #310000, processed 2941737 words, keeping 102833 word types\n",
      "2018-05-01 17:22:49,346 : INFO : PROGRESS: at sentence #320000, processed 3037057 words, keeping 104804 word types\n",
      "2018-05-01 17:22:49,377 : INFO : PROGRESS: at sentence #330000, processed 3132140 words, keeping 106742 word types\n",
      "2018-05-01 17:22:49,407 : INFO : PROGRESS: at sentence #340000, processed 3227270 words, keeping 108627 word types\n",
      "2018-05-01 17:22:49,443 : INFO : PROGRESS: at sentence #350000, processed 3322552 words, keeping 110503 word types\n",
      "2018-05-01 17:22:49,466 : INFO : collected 112090 word types from a corpus of 3402687 raw words and 358429 sentences\n",
      "2018-05-01 17:22:49,468 : INFO : Loading a fresh vocabulary\n",
      "2018-05-01 17:22:49,554 : INFO : min_count=5 retains 21885 unique words (19% of original 112090, drops 90205)\n",
      "2018-05-01 17:22:49,555 : INFO : min_count=5 leaves 3277346 word corpus (96% of original 3402687, drops 125341)\n",
      "2018-05-01 17:22:49,620 : INFO : deleting the raw counts dictionary of 112090 items\n",
      "2018-05-01 17:22:49,625 : INFO : sample=0.001 downsamples 33 most-common words\n",
      "2018-05-01 17:22:49,626 : INFO : downsampling leaves estimated 2630211 word corpus (80.3% of prior 3277346)\n",
      "2018-05-01 17:22:49,703 : INFO : estimated required memory for 21885 words and 200 dimensions: 45958500 bytes\n",
      "2018-05-01 17:22:49,703 : INFO : resetting layer weights\n",
      "2018-05-01 17:22:50,020 : INFO : training model with 3 workers on 21885 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-05-01 17:22:51,037 : INFO : EPOCH 1 - PROGRESS: at 33.19% examples, 870676 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:22:52,051 : INFO : EPOCH 1 - PROGRESS: at 61.41% examples, 800484 words/s, in_qsize 5, out_qsize 1\n",
      "2018-05-01 17:22:53,055 : INFO : EPOCH 1 - PROGRESS: at 96.07% examples, 836407 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-01 17:22:53,132 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-01 17:22:53,133 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-01 17:22:53,138 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-01 17:22:53,139 : INFO : EPOCH - 1 : training on 3402687 raw words (2630195 effective words) took 3.1s, 847062 effective words/s\n",
      "2018-05-01 17:22:54,155 : INFO : EPOCH 2 - PROGRESS: at 33.19% examples, 866204 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:22:55,159 : INFO : EPOCH 2 - PROGRESS: at 62.58% examples, 818055 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:22:56,160 : INFO : EPOCH 2 - PROGRESS: at 98.41% examples, 859240 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:22:56,187 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-01 17:22:56,190 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-01 17:22:56,199 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-01 17:22:56,200 : INFO : EPOCH - 2 : training on 3402687 raw words (2630552 effective words) took 3.1s, 861598 effective words/s\n",
      "2018-05-01 17:22:57,211 : INFO : EPOCH 3 - PROGRESS: at 39.36% examples, 1034246 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:22:58,213 : INFO : EPOCH 3 - PROGRESS: at 78.13% examples, 1025949 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:22:58,767 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-01 17:22:58,769 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-01 17:22:58,774 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-01 17:22:58,776 : INFO : EPOCH - 3 : training on 3402687 raw words (2630769 effective words) took 2.6s, 1025335 effective words/s\n",
      "2018-05-01 17:22:59,787 : INFO : EPOCH 4 - PROGRESS: at 39.65% examples, 1039811 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:23:00,788 : INFO : EPOCH 4 - PROGRESS: at 79.90% examples, 1048744 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:23:01,275 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-01 17:23:01,276 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-01 17:23:01,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-01 17:23:01,283 : INFO : EPOCH - 4 : training on 3402687 raw words (2629813 effective words) took 2.5s, 1052652 effective words/s\n",
      "2018-05-01 17:23:02,297 : INFO : EPOCH 5 - PROGRESS: at 40.83% examples, 1068683 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:23:03,299 : INFO : EPOCH 5 - PROGRESS: at 86.40% examples, 1131376 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-01 17:23:03,671 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-01 17:23:03,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-01 17:23:03,680 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-01 17:23:03,681 : INFO : EPOCH - 5 : training on 3402687 raw words (2629276 effective words) took 2.4s, 1100690 effective words/s\n",
      "2018-05-01 17:23:03,682 : INFO : training on a 17013435 raw words (13150605 effective words) took 13.7s, 962667 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# train word2vec on titles\n",
    "word2vec_size = 200\n",
    "train_titles = [title.split() for title in train_data[:,1]]\n",
    "word2vec_model = gensim.models.Word2Vec(train_titles, min_count=5, size=word2vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_vectorize_data(filename, word2vec_model, saveToDisk=False, test=False, word2vec_size=200):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    filepath = join(data_path, filename)\n",
    "    output_filepath = join(data_path, 'text_word2vec_features_' + filename)\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as tsv_file:\n",
    "        for line in tsv_file:\n",
    "            row = line.rstrip().split('\\t')\n",
    "            words = [token for token in row[1].split(' ') if token in word2vec_model.wv.vocab]\n",
    "            \n",
    "            w2v_words = np.zeros(word2vec_size)\n",
    "            if len(words) != 0:\n",
    "                w2v_words = word2vec_model[words]\n",
    "                w2v_words = np.mean(w2v_words, axis=0)\n",
    "            X.append(w2v_words)\n",
    "            Y.append(row[2])\n",
    "\n",
    "        X = np.array(X)\n",
    "        \n",
    "        if not test:\n",
    "            Y = le.transform(Y)\n",
    "            print (X.shape, Y.shape)\n",
    "        else:\n",
    "            Y = np.zeros(len(Y))\n",
    "        \n",
    "        if saveToDisk:\n",
    "            Y2 = Y.reshape((Y.shape[0], 1))\n",
    "            Z = np.concatenate((X, Y2), axis=1)\n",
    "            np.savetxt(output_filepath, Z, delimiter=',')\n",
    "        \n",
    "        else:\n",
    "            return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358429, 200) (358429,)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = word2vec_vectorize_data('cleaned_train.txt', word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 200) (10,)\n"
     ]
    }
   ],
   "source": [
    "word2vec_vectorize_data('cleaned_train_subset.txt', word2vec_model, saveToDisk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44629, 200) (44629,)\n"
     ]
    }
   ],
   "source": [
    "validation_X, validation_Y = word2vec_vectorize_data('cleaned_validation.txt', word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "s = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Macro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11890360989735102"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf2.predict(validation_X)\n",
    "f1_score(validation_Y, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22043962445943222"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(validation_Y, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class-wise Precision Recall on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                                            aaai       0.11      0.13      0.12       528\n",
      "                                           aamas       0.24      0.06      0.09       349\n",
      "                                             acc       0.00      0.00      0.00       109\n",
      "                                  acm_multimedia       0.14      0.08      0.10       379\n",
      "                               acm_trans._graph.       0.00      0.00      0.00         1\n",
      "                                           amcis       0.10      0.03      0.05       665\n",
      "                                            amia       0.33      0.03      0.06       121\n",
      "                                         asp-dac       0.20      0.01      0.01       319\n",
      "                                  bioinformatics       0.00      0.00      0.00        17\n",
      "                                             cdc       0.34      0.56      0.43       843\n",
      "                                             chi       0.13      0.09      0.10       418\n",
      "                          chi_extended_abstracts       0.15      0.17      0.16       568\n",
      "                                            cikm       0.09      0.20      0.12       377\n",
      "                                          cogsci       0.49      0.10      0.16       434\n",
      "                                          coling       0.16      0.16      0.16       367\n",
      "                                     commun._acm       0.08      0.01      0.02        68\n",
      "                                         compsac       0.10      0.01      0.02       296\n",
      "                            comput._graph._forum       0.00      0.00      0.00        19\n",
      "                                      comput._j.       0.00      0.00      0.00         2\n",
      "                         computer_communications       0.15      0.05      0.08       593\n",
      "                               computer_networks       0.15      0.18      0.17       446\n",
      "                                            corr       0.17      0.01      0.02       490\n",
      "                                            cvpr       0.01      0.01      0.01       187\n",
      "                                             dac       0.10      0.56      0.17       385\n",
      "                                            date       0.24      0.05      0.08       342\n",
      "                                            ecai       0.38      0.45      0.41       301\n",
      "                                            ecis       0.00      0.00      0.00       255\n",
      "                                            embc       0.00      0.00      0.00       318\n",
      "                encyclopedia_of_database_systems       0.15      0.03      0.06       852\n",
      "                                            etfa       0.40      0.01      0.02       259\n",
      "                                      eurospeech       0.04      0.03      0.03       289\n",
      "                                         eusipco       0.00      0.00      0.00         1\n",
      "                              expert_syst._appl.       0.59      0.07      0.13       286\n",
      "                                            focs       0.51      0.60      0.55       490\n",
      "                                            fskd       0.23      0.35      0.28       385\n",
      "                                 fundam._inform.       0.15      0.28      0.20      1323\n",
      "                                          fusion       0.33      0.14      0.20       981\n",
      "                                       fuzz-ieee       0.15      0.44      0.22       338\n",
      "                                           gecco       0.00      0.00      0.00       269\n",
      "                                        globecom       0.10      0.29      0.15      1662\n",
      "                                           hicss       0.12      0.01      0.02      1380\n",
      "                                           icalt       0.20      0.05      0.08       302\n",
      "                                          icarcv       0.03      0.11      0.04       211\n",
      "                                          icassp       0.01      0.00      0.01       272\n",
      "                                             icc       0.60      0.25      0.35       284\n",
      "                                           iccad       0.12      0.01      0.02       343\n",
      "                                            iccs       0.03      0.01      0.01       293\n",
      "                                            iccv       0.19      0.24      0.21       947\n",
      "                                           icdar       0.09      0.04      0.06       407\n",
      "                                            icde       0.55      0.52      0.54       435\n",
      "                                           icecs       0.03      0.00      0.01       540\n",
      "                                            icip       0.00      0.00      0.00       264\n",
      "                                            icis       0.19      0.02      0.04       210\n",
      "                                            icmc       0.07      0.02      0.03       456\n",
      "                                            icme       0.35      0.62      0.45      1793\n",
      "                                            icml       0.32      0.04      0.08       324\n",
      "                                            icnc       0.04      0.00      0.01       283\n",
      "                                            icpr       0.00      0.00      0.00        18\n",
      "                                            icra       0.40      0.06      0.10       396\n",
      "                                            icse       0.00      0.00      0.00         1\n",
      "                                           icslp       0.00      0.00      0.00         4\n",
      "                                   ieee_computer       0.00      0.00      0.00        19\n",
      "       ieee_congress_on_evolutionary_computation       0.09      0.12      0.10        42\n",
      "ieee_journal_on_selected_areas_in_communications       0.00      0.00      0.00         1\n",
      "                                   ieee_software       0.00      0.00      0.00        46\n",
      "                           ieee_trans._computers       0.56      0.82      0.66      1325\n",
      "                  ieee_trans._information_theory       0.06      0.03      0.04       752\n",
      "                    ieee_trans._knowl._data_eng.       0.20      0.06      0.10       543\n",
      "             ieee_trans._parallel_distrib._syst.       0.00      0.00      0.00        21\n",
      "         ieee_trans._pattern_anal._mach._intell.       0.19      0.07      0.10       690\n",
      "                       ieee_trans._software_eng.       0.28      0.57      0.37       358\n",
      "                                          igarss       0.34      0.72      0.46      1252\n",
      "                                           ijcai       0.10      0.16      0.12       449\n",
      "                                           ijcnn       0.18      0.07      0.10      1399\n",
      "                             inf._process._lett.       0.22      0.59      0.32       417\n",
      "                                       inf._sci.       0.33      0.12      0.17       889\n",
      "                                         infocom       0.00      0.00      0.00       323\n",
      "                             int._cmg_conference       0.36      0.40      0.38       460\n",
      "                                     interspeech       0.59      0.44      0.50       452\n",
      "                                           ipdps       0.00      0.00      0.00        32\n",
      "                                            iros       0.00      0.00      0.00         1\n",
      "                                            isbi       0.00      0.00      0.00         1\n",
      "                                           iscas       0.04      0.01      0.02       239\n",
      "                                            iscc       0.00      0.00      0.00       286\n",
      "                                            isit       0.44      0.50      0.47       461\n",
      "                                             itc       0.00      0.00      0.00         1\n",
      "                                          j._acm       0.20      0.02      0.04       588\n",
      "                    j._parallel_distrib._comput.       0.33      0.00      0.01       292\n",
      "                                   j._symb._log.       0.00      0.00      0.00         9\n",
      "                 journal_of_systems_and_software       0.05      0.29      0.08       301\n",
      "                                             kdd       0.14      0.32      0.20       898\n",
      "                                             lcn       0.06      0.01      0.02       404\n",
      "                                            lrec       0.06      0.07      0.06       577\n",
      "                          multimedia_tools_appl.       0.00      0.00      0.00        14\n",
      "                                      neuroimage       0.60      0.58      0.59       392\n",
      "                                            nips       0.20      0.27      0.23       351\n",
      "                                           pacis       0.36      0.02      0.03       299\n",
      "                             pattern_recognition       0.05      0.00      0.00       871\n",
      "                                           pdpta       0.11      0.46      0.17       292\n",
      "                                           pimrc       0.00      0.00      0.00        11\n",
      "                                           robio       0.23      0.21      0.22       311\n",
      "                                             sac       0.00      0.00      0.00        15\n",
      "                                 siam_j._comput.       0.14      0.04      0.06       271\n",
      "                                          sigcse       0.06      0.01      0.02       521\n",
      "                                           sigir       0.11      0.00      0.01       604\n",
      "                               sigmod_conference       0.12      0.08      0.10       781\n",
      "                                             smc       0.51      0.63      0.56       873\n",
      "\n",
      "                                     avg / total       0.21      0.22      0.18     44629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_Y, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, _ = word2vec_vectorize_data('cleaned_test.txt', word2vec_model, test=True)\n",
    "y_pred = clf2.predict(test_X)\n",
    "test_pred_Y = le.inverse_transform(y_pred)\n",
    "test_data = np.loadtxt(join(data_path, 'cleaned_test.txt'), dtype=str, delimiter='\\t')\n",
    "paper_ids = test_data[:,0]\n",
    "test_predictions = np.vstack((paper_ids, test_pred_Y)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    join(data_path,'text_word2vec_feature_predictions.txt'), \n",
    "    test_predictions, \n",
    "    delimiter='\\t',\n",
    "    fmt='%s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Word2Vec Embeddings with HIN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels_space_delimited):\n",
    "    encoding = np.zeros(len(labels))\n",
    "    labels_ = labels_space_delimited.split(' ')\n",
    "    label_indices = []\n",
    "    \n",
    "    for label in labels_:\n",
    "        try:\n",
    "            idx = le.transform([label])[0]\n",
    "            label_indices.append(idx)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for i in label_indices:\n",
    "        encoding[i] += 1.\n",
    "        \n",
    "    return encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hin_word2vec_vectorize_data(filename, word2vec_model, saveToDisk=False, test=False, word2vec_size=200):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    filepath = join(data_path, filename)\n",
    "    output_filepath = join(data_path, 'text_word2vec_hin_features_' + filename)\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as tsv_file:\n",
    "        for line in tsv_file:\n",
    "            row = line.rstrip().split('\\t')\n",
    "            words = [token for token in row[1].split(' ') if token in word2vec_model.wv.vocab]\n",
    "            \n",
    "            w2v_words = np.zeros(word2vec_size)\n",
    "            if len(words) != 0:\n",
    "                w2v_words = word2vec_model[words]\n",
    "                w2v_words = np.mean(w2v_words, axis=0)\n",
    "                \n",
    "            w2v_words = np.concatenate((w2v_words, one_hot_encode(row[4])))\n",
    "            X.append(w2v_words)\n",
    "            Y.append(row[2])\n",
    "\n",
    "        X = np.array(X)\n",
    "        \n",
    "        if not test:\n",
    "            Y = le.transform(Y)\n",
    "            print (X.shape, Y.shape)\n",
    "        else:\n",
    "            Y = np.zeros(len(Y))\n",
    "        \n",
    "        if saveToDisk:\n",
    "            Y2 = Y.reshape((Y.shape[0], 1))\n",
    "            Z = np.concatenate((X, Y2), axis=1)\n",
    "            np.savetxt(output_filepath, Z, delimiter=',')\n",
    "        \n",
    "        else:\n",
    "            return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358429, 316) (358429,)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = hin_word2vec_vectorize_data('cleaned_train.txt', word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 316) (10,)\n"
     ]
    }
   ],
   "source": [
    "hin_word2vec_vectorize_data('cleaned_train_subset.txt', word2vec_model, saveToDisk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44629, 316) (44629,)\n"
     ]
    }
   ],
   "source": [
    "validation_X, validation_Y = hin_word2vec_vectorize_data('cleaned_validation.txt', word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "s = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Macro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7887026160652078"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf2.predict(validation_X)\n",
    "f1_score(validation_Y, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9783772883102915"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(validation_Y, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class-wise Precision Recall on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                                            aaai       0.83      0.94      0.88       528\n",
      "                                           aamas       1.00      1.00      1.00       349\n",
      "                                             acc       1.00      1.00      1.00       109\n",
      "                                  acm_multimedia       1.00      1.00      1.00       379\n",
      "                               acm_trans._graph.       0.00      0.00      0.00         1\n",
      "                                           amcis       1.00      1.00      1.00       665\n",
      "                                            amia       1.00      1.00      1.00       121\n",
      "                                         asp-dac       1.00      1.00      1.00       319\n",
      "                                  bioinformatics       1.00      1.00      1.00        17\n",
      "                                             cdc       1.00      1.00      1.00       843\n",
      "                                             chi       1.00      1.00      1.00       418\n",
      "                          chi_extended_abstracts       1.00      1.00      1.00       568\n",
      "                                            cikm       0.72      0.86      0.79       377\n",
      "                                          cogsci       1.00      1.00      1.00       434\n",
      "                                          coling       0.99      0.96      0.98       367\n",
      "                                     commun._acm       0.00      0.00      0.00        68\n",
      "                                         compsac       1.00      1.00      1.00       296\n",
      "                            comput._graph._forum       0.00      0.00      0.00        19\n",
      "                                      comput._j.       0.00      0.00      0.00         2\n",
      "                         computer_communications       0.99      0.89      0.93       593\n",
      "                               computer_networks       1.00      1.00      1.00       446\n",
      "                                            corr       1.00      1.00      1.00       490\n",
      "                                            cvpr       1.00      0.97      0.99       187\n",
      "                                             dac       1.00      1.00      1.00       385\n",
      "                                            date       1.00      1.00      1.00       342\n",
      "                                            ecai       1.00      1.00      1.00       301\n",
      "                                            ecis       1.00      1.00      1.00       255\n",
      "                                            embc       1.00      1.00      1.00       318\n",
      "                encyclopedia_of_database_systems       1.00      0.94      0.97       852\n",
      "                                            etfa       1.00      1.00      1.00       259\n",
      "                                      eurospeech       1.00      1.00      1.00       289\n",
      "                                         eusipco       0.00      0.00      0.00         1\n",
      "                              expert_syst._appl.       1.00      1.00      1.00       286\n",
      "                                            focs       1.00      1.00      1.00       490\n",
      "                                            fskd       1.00      1.00      1.00       385\n",
      "                                 fundam._inform.       1.00      1.00      1.00      1323\n",
      "                                          fusion       1.00      1.00      1.00       981\n",
      "                                       fuzz-ieee       1.00      1.00      1.00       338\n",
      "                                           gecco       1.00      1.00      1.00       269\n",
      "                                        globecom       1.00      1.00      1.00      1662\n",
      "                                           hicss       1.00      1.00      1.00      1380\n",
      "                                           icalt       1.00      1.00      1.00       302\n",
      "                                          icarcv       1.00      0.99      1.00       211\n",
      "                                          icassp       0.82      0.79      0.80       272\n",
      "                                             icc       0.99      0.93      0.95       284\n",
      "                                           iccad       0.65      0.73      0.69       343\n",
      "                                            iccs       1.00      1.00      1.00       293\n",
      "                                            iccv       1.00      1.00      1.00       947\n",
      "                                           icdar       1.00      1.00      1.00       407\n",
      "                                            icde       1.00      1.00      1.00       435\n",
      "                                           icecs       1.00      1.00      1.00       540\n",
      "                                            icip       1.00      0.91      0.95       264\n",
      "                                            icis       1.00      1.00      1.00       210\n",
      "                                            icmc       1.00      1.00      1.00       456\n",
      "                                            icme       1.00      1.00      1.00      1793\n",
      "                                            icml       1.00      1.00      1.00       324\n",
      "                                            icnc       1.00      1.00      1.00       283\n",
      "                                            icpr       0.00      0.00      0.00        18\n",
      "                                            icra       1.00      1.00      1.00       396\n",
      "                                            icse       0.00      0.00      0.00         1\n",
      "                                           icslp       0.00      0.00      0.00         4\n",
      "                                   ieee_computer       0.00      0.00      0.00        19\n",
      "       ieee_congress_on_evolutionary_computation       0.57      0.10      0.16        42\n",
      "ieee_journal_on_selected_areas_in_communications       0.00      0.00      0.00         1\n",
      "                                   ieee_software       0.00      0.00      0.00        46\n",
      "                           ieee_trans._computers       1.00      1.00      1.00      1325\n",
      "                  ieee_trans._information_theory       1.00      1.00      1.00       752\n",
      "                    ieee_trans._knowl._data_eng.       1.00      1.00      1.00       543\n",
      "             ieee_trans._parallel_distrib._syst.       0.00      0.00      0.00        21\n",
      "         ieee_trans._pattern_anal._mach._intell.       1.00      1.00      1.00       690\n",
      "                       ieee_trans._software_eng.       1.00      1.00      1.00       358\n",
      "                                          igarss       1.00      1.00      1.00      1252\n",
      "                                           ijcai       1.00      1.00      1.00       449\n",
      "                                           ijcnn       1.00      1.00      1.00      1399\n",
      "                             inf._process._lett.       1.00      1.00      1.00       417\n",
      "                                       inf._sci.       0.74      0.97      0.84       889\n",
      "                                         infocom       1.00      1.00      1.00       323\n",
      "                             int._cmg_conference       1.00      1.00      1.00       460\n",
      "                                     interspeech       0.92      0.87      0.89       452\n",
      "                                           ipdps       0.00      0.00      0.00        32\n",
      "                                            iros       0.00      0.00      0.00         1\n",
      "                                            isbi       0.00      0.00      0.00         1\n",
      "                                           iscas       1.00      0.99      1.00       239\n",
      "                                            iscc       1.00      0.99      1.00       286\n",
      "                                            isit       1.00      1.00      1.00       461\n",
      "                                             itc       0.00      0.00      0.00         1\n",
      "                                          j._acm       1.00      1.00      1.00       588\n",
      "                    j._parallel_distrib._comput.       1.00      1.00      1.00       292\n",
      "                                   j._symb._log.       0.00      0.00      0.00         9\n",
      "                 journal_of_systems_and_software       1.00      1.00      1.00       301\n",
      "                                             kdd       1.00      1.00      1.00       898\n",
      "                                             lcn       1.00      1.00      1.00       404\n",
      "                                            lrec       1.00      1.00      1.00       577\n",
      "                          multimedia_tools_appl.       0.00      0.00      0.00        14\n",
      "                                      neuroimage       1.00      1.00      1.00       392\n",
      "                                            nips       0.98      0.82      0.89       351\n",
      "                                           pacis       0.62      0.83      0.71       299\n",
      "                             pattern_recognition       1.00      1.00      1.00       871\n",
      "                                           pdpta       1.00      1.00      1.00       292\n",
      "                                           pimrc       0.00      0.00      0.00        11\n",
      "                                           robio       1.00      1.00      1.00       311\n",
      "                                             sac       0.00      0.00      0.00        15\n",
      "                                 siam_j._comput.       1.00      0.97      0.99       271\n",
      "                                          sigcse       1.00      1.00      1.00       521\n",
      "                                           sigir       1.00      1.00      1.00       604\n",
      "                               sigmod_conference       1.00      1.00      1.00       781\n",
      "                                             smc       1.00      1.00      1.00       873\n",
      "\n",
      "                                     avg / total       0.98      0.98      0.98     44629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_Y, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, _ = hin_word2vec_vectorize_data('cleaned_test.txt', word2vec_model, test=True)\n",
    "y_pred = clf2.predict(test_X)\n",
    "test_pred_Y = le.inverse_transform(y_pred)\n",
    "test_data = np.loadtxt(join(data_path, 'cleaned_test.txt'), dtype=str, delimiter='\\t')\n",
    "paper_ids = test_data[:,0]\n",
    "test_predictions = np.vstack((paper_ids, test_pred_Y)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    join(data_path,'text_word2vec_hin_feature_predictions.txt'), \n",
    "    test_predictions, \n",
    "    delimiter='\\t',\n",
    "    fmt='%s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 TF-IDF Weighted Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358429, 112092) (358429,)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, train_vectorizer, train_transformer = vectorize_data('cleaned_train.txt', ngram_range=(1, 1), tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 112092) (10,)\n"
     ]
    }
   ],
   "source": [
    "vectorize_data('cleaned_train_subset.txt', ngram_range=(1, 1), saveToDisk=True, train=False, vectorizer=train_vectorizer, transformer=train_transformer, tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44629, 112092) (44629,)\n"
     ]
    }
   ],
   "source": [
    "validation_X, validation_Y = vectorize_data('cleaned_validation.txt', ngram_range=(1, 1), train=False, vectorizer=train_vectorizer, transformer=train_transformer, tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "s = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Macro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22451030875267966"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf2.predict(validation_X)\n",
    "f1_score(validation_Y, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro F1 score on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31983687736673466"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(validation_Y, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class-wise Precision Recall on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                                            aaai       0.09      0.07      0.08       528\n",
      "                                           aamas       0.32      0.40      0.36       349\n",
      "                                             acc       0.05      0.03      0.04       109\n",
      "                                  acm_multimedia       0.23      0.22      0.23       379\n",
      "                               acm_trans._graph.       0.00      0.00      0.00         1\n",
      "                                           amcis       0.31      0.24      0.27       665\n",
      "                                            amia       0.44      0.60      0.51       121\n",
      "                                         asp-dac       0.16      0.13      0.14       319\n",
      "                                  bioinformatics       0.06      0.06      0.06        17\n",
      "                                             cdc       0.43      0.67      0.53       843\n",
      "                                             chi       0.26      0.24      0.25       418\n",
      "                          chi_extended_abstracts       0.39      0.36      0.38       568\n",
      "                                            cikm       0.12      0.11      0.12       377\n",
      "                                          cogsci       0.47      0.61      0.53       434\n",
      "                                          coling       0.34      0.33      0.33       367\n",
      "                                     commun._acm       0.09      0.10      0.10        68\n",
      "                                         compsac       0.16      0.12      0.14       296\n",
      "                            comput._graph._forum       0.12      0.05      0.07        19\n",
      "                                      comput._j.       0.00      0.00      0.00         2\n",
      "                         computer_communications       0.26      0.18      0.22       593\n",
      "                               computer_networks       0.29      0.22      0.25       446\n",
      "                                            corr       0.22      0.16      0.19       490\n",
      "                                            cvpr       0.05      0.03      0.04       187\n",
      "                                             dac       0.29      0.18      0.22       385\n",
      "                                            date       0.37      0.36      0.37       342\n",
      "                                            ecai       0.41      0.56      0.48       301\n",
      "                                            ecis       0.30      0.24      0.27       255\n",
      "                                            embc       0.21      0.13      0.16       318\n",
      "                encyclopedia_of_database_systems       0.19      0.14      0.16       852\n",
      "                                            etfa       0.23      0.16      0.19       259\n",
      "                                      eurospeech       0.13      0.04      0.06       289\n",
      "                                         eusipco       0.00      0.00      0.00         1\n",
      "                              expert_syst._appl.       0.40      0.47      0.43       286\n",
      "                                            focs       0.47      0.72      0.57       490\n",
      "                                            fskd       0.44      0.37      0.40       385\n",
      "                                 fundam._inform.       0.20      0.17      0.18      1323\n",
      "                                          fusion       0.38      0.29      0.33       981\n",
      "                                       fuzz-ieee       0.49      0.54      0.52       338\n",
      "                                           gecco       0.02      0.01      0.01       269\n",
      "                                        globecom       0.26      0.24      0.25      1662\n",
      "                                           hicss       0.19      0.14      0.16      1380\n",
      "                                           icalt       0.22      0.15      0.17       302\n",
      "                                          icarcv       0.23      0.23      0.23       211\n",
      "                                          icassp       0.15      0.10      0.12       272\n",
      "                                             icc       0.44      0.76      0.56       284\n",
      "                                           iccad       0.21      0.17      0.19       343\n",
      "                                            iccs       0.13      0.05      0.08       293\n",
      "                                            iccv       0.28      0.27      0.28       947\n",
      "                                           icdar       0.17      0.14      0.15       407\n",
      "                                            icde       0.44      0.80      0.57       435\n",
      "                                           icecs       0.16      0.09      0.12       540\n",
      "                                            icip       0.17      0.19      0.18       264\n",
      "                                            icis       0.12      0.12      0.12       210\n",
      "                                            icmc       0.07      0.03      0.04       456\n",
      "                                            icme       0.41      0.52      0.46      1793\n",
      "                                            icml       0.31      0.39      0.34       324\n",
      "                                            icnc       0.20      0.07      0.11       283\n",
      "                                            icpr       0.00      0.00      0.00        18\n",
      "                                            icra       0.37      0.43      0.40       396\n",
      "                                            icse       0.00      0.00      0.00         1\n",
      "                                           icslp       0.00      0.00      0.00         4\n",
      "                                   ieee_computer       0.00      0.00      0.00        19\n",
      "       ieee_congress_on_evolutionary_computation       0.15      0.05      0.07        42\n",
      "ieee_journal_on_selected_areas_in_communications       0.00      0.00      0.00         1\n",
      "                                   ieee_software       0.04      0.02      0.03        46\n",
      "                           ieee_trans._computers       0.51      0.91      0.65      1325\n",
      "                  ieee_trans._information_theory       0.20      0.17      0.18       752\n",
      "                    ieee_trans._knowl._data_eng.       0.30      0.25      0.27       543\n",
      "             ieee_trans._parallel_distrib._syst.       0.00      0.00      0.00        21\n",
      "         ieee_trans._pattern_anal._mach._intell.       0.27      0.28      0.28       690\n",
      "                       ieee_trans._software_eng.       0.53      0.70      0.60       358\n",
      "                                          igarss       0.46      0.72      0.56      1252\n",
      "                                           ijcai       0.33      0.30      0.31       449\n",
      "                                           ijcnn       0.31      0.20      0.24      1399\n",
      "                             inf._process._lett.       0.44      0.68      0.54       417\n",
      "                                       inf._sci.       0.36      0.42      0.39       889\n",
      "                                         infocom       0.06      0.05      0.05       323\n",
      "                             int._cmg_conference       0.39      0.61      0.48       460\n",
      "                                     interspeech       0.50      0.74      0.60       452\n",
      "                                           ipdps       0.07      0.03      0.04        32\n",
      "                                            iros       0.00      0.00      0.00         1\n",
      "                                            isbi       0.00      0.00      0.00         1\n",
      "                                           iscas       0.18      0.24      0.21       239\n",
      "                                            iscc       0.15      0.16      0.16       286\n",
      "                                            isit       0.44      0.66      0.53       461\n",
      "                                             itc       0.00      0.00      0.00         1\n",
      "                                          j._acm       0.24      0.26      0.25       588\n",
      "                    j._parallel_distrib._comput.       0.19      0.13      0.15       292\n",
      "                                   j._symb._log.       0.00      0.00      0.00         9\n",
      "                 journal_of_systems_and_software       0.19      0.21      0.20       301\n",
      "                                             kdd       0.22      0.14      0.17       898\n",
      "                                             lcn       0.19      0.13      0.16       404\n",
      "                                            lrec       0.14      0.10      0.12       577\n",
      "                          multimedia_tools_appl.       0.00      0.00      0.00        14\n",
      "                                      neuroimage       0.52      0.78      0.62       392\n",
      "                                            nips       0.33      0.34      0.34       351\n",
      "                                           pacis       0.20      0.18      0.19       299\n",
      "                             pattern_recognition       0.14      0.11      0.12       871\n",
      "                                           pdpta       0.29      0.35      0.32       292\n",
      "                                           pimrc       0.33      0.09      0.14        11\n",
      "                                           robio       0.33      0.20      0.25       311\n",
      "                                             sac       0.08      0.07      0.07        15\n",
      "                                 siam_j._comput.       0.10      0.07      0.08       271\n",
      "                                          sigcse       0.12      0.13      0.12       521\n",
      "                                           sigir       0.12      0.15      0.14       604\n",
      "                               sigmod_conference       0.13      0.09      0.10       781\n",
      "                                             smc       0.48      0.78      0.59       873\n",
      "\n",
      "                                     avg / total       0.29      0.32      0.29     44629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_Y, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, _ = vectorize_data('cleaned_test.txt', ngram_range=(1, 1), test=True, train=False, vectorizer=train_vectorizer, transformer=train_transformer, tfidf=False)\n",
    "y_pred = clf2.predict(test_X)\n",
    "test_pred_Y = le.inverse_transform(y_pred)\n",
    "test_data = np.loadtxt(join(data_path, 'cleaned_test.txt'), dtype=str, delimiter='\\t')\n",
    "paper_ids = test_data[:,0]\n",
    "test_predictions = np.vstack((paper_ids, test_pred_Y)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    join(data_path,'text_feature_tfidf_predictions.txt'), \n",
    "    test_predictions, \n",
    "    delimiter='\\t',\n",
    "    fmt='%s'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
